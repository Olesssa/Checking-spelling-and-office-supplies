#%%
# #–ó–∞–≥—Ä—É–∑–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
import pandas as pd
import nltk
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import numpy as np

nltk.download('punkt')
nltk.download('punkt_tab')
# %%
# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
excel_path = r'C:\Users\–û–ª–µ—Å—è\Desktop\kanclerisms_labeled (2).xlsx'
html_path = r'C:\Users\–û–ª–µ—Å—è\Desktop\first model\template.html'

# –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Å –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–∞–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏
df = pd.read_excel(excel_path)

# %%
#–°–æ–∑–¥–∞–µ–º —Ö –∏ y
X = df['phrase'].astype(str)
y = df['label'].astype(str)

# –î–µ–ª–∏–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1616)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã TF-IDF
vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5))
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)
# %%
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import numpy as np

# –î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ C
# —á–µ–º –±–æ–ª—å—à–µ C, —Ç–µ–º —Å–ª–∞–±–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
C_values = np.logspace(-3, 3, 15)  # –æ—Ç 0.001 –¥–æ 1000 (15 —Ç–æ—á–µ–∫)

# --- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ---
logreg_results = {
    'C': C_values,
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': []
}

#–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö C
for C in C_values:
    clf = LogisticRegression(C=C, max_iter=500, solver='liblinear', random_state=1630)
    clf.fit(X_train_vec, y_train)
    y_pred = clf.predict(X_test_vec)

    logreg_results['accuracy'].append(accuracy_score(y_test, y_pred))
    logreg_results['precision'].append(precision_score(y_test, y_pred, average='macro'))
    logreg_results['recall'].append(recall_score(y_test, y_pred, average='macro'))
    logreg_results['f1'].append(f1_score(y_test, y_pred, average='macro'))

best_C = 1.5  # –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
logreg = LogisticRegression(C=best_C, max_iter=500, solver='liblinear', random_state=1630)
logreg.fit(X_train_vec, y_train)
y_pred_logreg = logreg.predict(X_test_vec)

accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
precision_logreg = precision_score(y_test, y_pred_logreg, average='macro', zero_division=0)
recall_logreg = recall_score(y_test, y_pred_logreg, average='macro', zero_division=0)
f1_logreg = f1_score(y_test, y_pred_logreg, average='macro', zero_division=0)

print(f"Accuracy: {accuracy_logreg:.4f}")
print(f"Precision: {precision_logreg:.4f}")
print(f"Recall: {recall_logreg:.4f}")
print(f"F1-score: {f1_logreg:.4f}")
print()
# %%
from sklearn.ensemble import RandomForestClassifier

# –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è Random Forest
rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10]
}

rf = RandomForestClassifier(random_state=1630)
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='f1_macro', n_jobs=-1)
rf_grid.fit(X_train_vec, y_train)

print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Random Forest: {rf_grid.best_params_}")

best_rf = rf_grid.best_estimator_
y_pred_rf = best_rf.predict(X_test_vec)

accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf, average='macro', zero_division=0)
recall_rf = recall_score(y_test, y_pred_rf, average='macro', zero_division=0)
f1_rf = f1_score(y_test, y_pred_rf, average='macro', zero_division=0)

print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")
print()

# %%
print("=== Support Vector Machine ===")

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SVM (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
scaler = StandardScaler(with_mean=False)  # with_mean=False –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
X_train_scaled = scaler.fit_transform(X_train_vec)
X_test_scaled = scaler.transform(X_test_vec)

# –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è SVM
svm_params = {
    'C': [0.1, 1],
    'kernel': ['linear', 'rbf']
}

svm = SVC(random_state=1630)
svm_grid = GridSearchCV(svm, svm_params, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1)
svm_grid.fit(X_train_scaled, y_train)

print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã SVM: {svm_grid.best_params_}")

best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(X_test_scaled)

accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm, average='macro', zero_division=0)
recall_svm = recall_score(y_test, y_pred_svm, average='macro', zero_division=0)
f1_svm = f1_score(y_test, y_pred_svm, average='macro', zero_division=0)

print(f"Accuracy: {accuracy_svm:.4f}")
print(f"Precision: {precision_svm:.4f}")
print(f"Recall: {recall_svm:.4f}")
print(f"F1-score: {f1_svm:.4f}")
print()

#%%
#%pip install matplotlib

#%%
import matplotlib.pyplot as plt

# %%
print("=== –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===")
models_comparison = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest', 'SVM'],
    'Accuracy': [accuracy_logreg, accuracy_rf, accuracy_svm],
    'Precision': [precision_logreg, precision_rf, precision_svm],
    'Recall': [recall_logreg, recall_rf, recall_svm],
    'F1-score': [f1_logreg, f1_rf, f1_svm]
})

print(models_comparison.round(4))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
plt.figure(figsize=(12, 8))
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
x = np.arange(len(metrics))
width = 0.25

plt.bar(x - width, models_comparison.iloc[0, 1:], width, label='Logistic Regression', alpha=0.8)
plt.bar(x, models_comparison.iloc[1, 1:], width, label='Random Forest', alpha=0.8)
plt.bar(x + width, models_comparison.iloc[2, 1:], width, label='SVM', alpha=0.8)

plt.xlabel('Metrics')
plt.ylabel('Score')
plt.title('Comparison of Model Performance')
plt.xticks(x, metrics)
plt.legend()
plt.grid(True, alpha=0.3)
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

# –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ F1-score
best_model_idx = models_comparison['F1-score'].idxmax()
best_model_name = models_comparison.loc[best_model_idx, 'Model']
best_model_score = models_comparison.loc[best_model_idx, 'F1-score']

print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name} —Å F1-score = {best_model_score:.4f}")
# %%
print("=== –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===")
models_comparison = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest', 'SVM'],
    'Accuracy': [accuracy_logreg, accuracy_rf, accuracy_svm],
    'Precision': [precision_logreg, precision_rf, precision_svm],
    'Recall': [recall_logreg, recall_rf, recall_svm],
    'F1-score': [f1_logreg, f1_rf, f1_svm]
})

print(models_comparison.round(4))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
plt.figure(figsize=(14, 9))

# –°–∏–Ω—è—è —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞
colors = ['#1f77b4', '#3498db', '#2980b9']  # –û—Ç—Ç–µ–Ω–∫–∏ —Å–∏–Ω–µ–≥–æ
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
x = np.arange(len(metrics))
width = 0.25

# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –¥–∏–∑–∞–π–Ω–æ–º
bars1 = plt.bar(x - width, models_comparison.iloc[0, 1:], width,
                label='Logistic Regression',
                color=colors[0],
                edgecolor='white',
                linewidth=1.5,
                alpha=0.9)

bars2 = plt.bar(x, models_comparison.iloc[1, 1:], width,
                label='Random Forest',
                color=colors[1],
                edgecolor='white',
                linewidth=1.5,
                alpha=0.9)

bars3 = plt.bar(x + width, models_comparison.iloc[2, 1:], width,
                label='SVM',
                color=colors[2],
                edgecolor='white',
                linewidth=1.5,
                alpha=0.9)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
def add_value_labels(bars):
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom',
                fontsize=10, fontweight='bold', color='#2c3e50')

add_value_labels(bars1)
add_value_labels(bars2)
add_value_labels(bars3)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞
plt.xlabel('–ú–µ—Ç—Ä–∏–∫–∏', fontsize=12, fontweight='bold', color='#2c3e50')
plt.ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ', fontsize=12, fontweight='bold', color='#2c3e50')
plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏',
          fontsize=14, fontweight='bold', pad=20, color='#2c3e50')
plt.xticks(x, metrics, fontsize=11, fontweight='bold')
plt.yticks(fontsize=10)

# –õ–µ–≥–µ–Ω–¥–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –¥–∏–∑–∞–π–Ω–æ–º
legend = plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15),
                   ncol=3, frameon=True, fancybox=True,
                   shadow=True, fontsize=11)
legend.get_frame().set_facecolor('#ecf0f1')
legend.get_frame().set_alpha(0.9)

# –°–µ—Ç–∫–∞
plt.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)
plt.ylim(0, 1.1)

# –§–æ–Ω –∏ –≥—Ä–∞–Ω–∏—Ü—ã
plt.gca().set_facecolor('#f8f9fa')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.gca().spines['left'].set_color('#bdc3c7')
plt.gca().spines['bottom'].set_color('#bdc3c7')

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–π –ª–∏–Ω–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ 0.5 –¥–ª—è –æ—Ä–∏–µ–Ω—Ç–∏—Ä–∞
plt.axhline(y=0.5, color='#e74c3c', linestyle=':', alpha=0.7, linewidth=1)

plt.tight_layout()
plt.show()

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è - —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –º–µ—Ç—Ä–∏–∫
plt.figure(figsize=(10, 6))
metrics_data = models_comparison.set_index('Model').T

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã
plt.imshow(metrics_data, cmap='Blues', aspect='auto', vmin=0, vmax=1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π
plt.xticks(range(len(metrics_data.columns)), metrics_data.columns, rotation=45, ha='right')
plt.yticks(range(len(metrics_data.index)), metrics_data.index)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –≤ —è—á–µ–π–∫–∏
for i in range(len(metrics_data.index)):
    for j in range(len(metrics_data.columns)):
        plt.text(j, i, f'{metrics_data.iloc[i, j]:.3f}',
                ha='center', va='center', fontweight='bold',
                color='white' if metrics_data.iloc[i, j] > 0.6 else 'black')

plt.colorbar(label='–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏')
plt.title('–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π', fontsize=14, fontweight='bold', pad=20)
plt.xlabel('–ú–æ–¥–µ–ª–∏', fontweight='bold')
plt.ylabel('–ú–µ—Ç—Ä–∏–∫–∏', fontweight='bold')
plt.tight_layout()
plt.show()

# –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ F1-score
best_model_idx = models_comparison['F1-score'].idxmax()
best_model_name = models_comparison.loc[best_model_idx, 'Model']
best_model_score = models_comparison.loc[best_model_idx, 'F1-score']

print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name} —Å F1-score = {best_model_score:.4f}")

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
best_model_row = models_comparison.loc[best_model_idx]
print(f"üìä –ü–æ–ª–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏:")
print(f"   ‚Ä¢ Accuracy: {best_model_row['Accuracy']:.4f}")
print(f"   ‚Ä¢ Precision: {best_model_row['Precision']:.4f}")
print(f"   ‚Ä¢ Recall: {best_model_row['Recall']:.4f}")
print(f"   ‚Ä¢ F1-score: {best_model_row['F1-score']:.4f}")
# %%
