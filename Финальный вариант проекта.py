#%%
# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ —Ä–µ—Å—É—Ä—Å—ã
import pandas as pd
import nltk
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import numpy as np
from spellchecker import SpellChecker

# –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã NLTK
nltk.download('punkt')
nltk.download('punkt_tab')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏
spell = SpellChecker(language='ru')

#%%
# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
excel_path = r'C:\Users\–û–ª–µ—Å—è\Desktop\kanclerisms_labeled (2).xlsx'
html_path = r'C:\Users\–û–ª–µ—Å—è\Desktop\first model\template.html'

# –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Ñ–∞–π–ª —Å –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–∞–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏
df = pd.read_excel(excel_path)
# %%
# 3. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ HTML
with open(html_path, 'r', encoding='utf-8') as f:
    html_content = f.read()

soup = BeautifulSoup(html_content, 'html.parser')
text = soup.get_text(separator=' ')
# %%
from sklearn.ensemble import RandomForestClassifier
# 4. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å randomforrest —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤
X = df['phrase'].astype(str)
y = df['label'].astype(str)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1616)

vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5))
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

rf = RandomForestClassifier(random_state=1630, max_depth = 20, min_samples_split = 10, n_estimators = 200)
rf.fit(X_train_vec, y_train)
# %%
#–ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º—ã –∏ –æ—Ä—Ñ–æ–≥—Ä–∞–º–º—ã
def highlight_all(html, threshold=0.7):
    soup = BeautifulSoup(html, 'html.parser')
    text_nodes = soup.find_all(string=True)

    for node in text_nodes:
        words = nltk.word_tokenize(node, language='russian')
        new_node = node

        for w in set(words):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Å–ª–æ–≤–∞ —Å –Ω–µ–∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
            if len(w) < 3 or not w.isalpha():
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º —á–µ—Ä–µ–∑ ML –º–æ–¥–µ–ª—å
            prob = rf.predict_proba(vectorizer.transform([w]))[0]
            idx = list(rf.classes_).index('–∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º')
            is_kanclerism = prob[idx] > threshold

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É
            is_spelling_error = w.lower() not in spell

            # –í—ã–±–∏—Ä–∞–µ–º —Ü–≤–µ—Ç
            if is_kanclerism and is_spelling_error:
                color = '#ffb347'  #–æ—Ä–∞–Ω–∂–µ–≤—ã–π (–æ–±–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞)
            elif is_kanclerism:
                color = '#ffec99'  #–∂—ë–ª—Ç—ã–π (–∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º)
            elif is_spelling_error:
                color = '#ff9999'  #–∫—Ä–∞—Å–Ω—ã–π (–æ—à–∏–±–∫–∞)
            else:
                continue  # –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã–¥–µ–ª—è–µ–º

            # –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º —Å–ª–æ–≤–æ
            new_node = new_node.replace(
                w, f'<mark style="background-color:{color};">{w}</mark>'
            )

        node.replace_with(BeautifulSoup(new_node, 'html.parser'))

    return str(soup)


highlighted_html = highlight_all(html_content)
# %%
output_path = r"C:\Users\–û–ª–µ—Å—è\Desktop\content\result1.html"

# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –µ—â—ë –Ω–µ—Ç
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
with open(output_path, "w", encoding="utf-8") as f:
    f.write(highlighted_html)

# %%
